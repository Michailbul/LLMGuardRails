{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation with Vector Stores\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "## ^^ Comment out if you want to see the pip install process\n",
    "\n",
    "## Necessary for Colab, not necessary for course environment\n",
    "# %pip install -q langchain langchain-nvidia-ai-endpoints gradio rich\n",
    "# %pip install -q arxiv pymupdf faiss-cpu\n",
    "    \n",
    "## If you're in colab and encounter a typing-extensions issue,\n",
    "##  restart your runtime and try again\n",
    "from langchain_nvidia_ai_endpoints._common import NVEModel\n",
    "\n",
    "from functools import partial\n",
    "from rich.console import Console\n",
    "from rich.style import Style\n",
    "from rich.theme import Theme\n",
    "\n",
    "console = Console()\n",
    "base_style = Style(color=\"#76B900\", bold=True)\n",
    "pprint = partial(console.print, style=base_style)\n",
    "\n",
    "from getpass import getpass\n",
    "import requests\n",
    "import os\n",
    "\n",
    "hard_reset = False  ## <-- Set to True if you want to reset your NVIDIA_API_KEY\n",
    "while \"nvapi-\" not in os.environ.get(\"NVIDIA_API_KEY\", \"\") or hard_reset:\n",
    "    try: \n",
    "        assert not hard_reset\n",
    "        response = requests.get(\"http://docker_router:8070/get_key\").json()\n",
    "        assert response.get('nvapi_key')\n",
    "    except: response = {'nvapi_key' : getpass(\"NVIDIA API Key: \")}\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = response.get(\"nvapi_key\")\n",
    "    try: requests.post(\"http://docker_router:8070/set_key/\", json={'nvapi_key' : os.environ[\"NVIDIA_API_KEY\"]}).json()\n",
    "    except: pass\n",
    "    hard_reset = False\n",
    "    if \"nvapi-\" not in os.environ.get(\"NVIDIA_API_KEY\", \"\"):\n",
    "        print(\"[!] API key assignment failed. Make sure it starts with `nvapi-` as generated from the model pages.\")\n",
    "\n",
    "print(f\"Retrieved NVIDIA_API_KEY beginning with \\\"{os.environ.get('NVIDIA_API_KEY')[:9]}...\\\"\")\n",
    "from langchain_nvidia_ai_endpoints._common import NVEModel\n",
    "NVEModel().available_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = 'ls__ae038ddbc62c4af090ecc776a2e1254b'\n",
    "os.environ['LANGCHAIN_PROJECT'] = \"researchAssistant\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = [  ## This conversation was generated partially by an AI system, and modified to exhibit desirable properties\n",
    "    \"[User]  Hello! My name is Beras, and I'm a big blue bear! Can you please tell me about the rocky mountains?\",\n",
    "    \"[Agent] The Rocky Mountains are a beautiful and majestic range of mountains that stretch across North America\",\n",
    "    \"[Beras] Wow, that sounds amazing! Ive never been to the Rocky Mountains before, but Ive heard many great things about them.\",\n",
    "    \"[Agent] I hope you get to visit them someday, Beras! It would be a great adventure for you!\"\n",
    "    \"[Beras] Thank you for the suggestion! Ill definitely keep it in mind for the future.\",\n",
    "    \"[Agent] In the meantime, you can learn more about the Rocky Mountains by doing some research online or watching documentaries about them.\"\n",
    "    \"[Beras] I live in the arctic, so I'm not used to the warm climate there. I was just curious, ya know!\",\n",
    "    \"[Agent] Absolutely! Lets continue the conversation and explore more about the Rocky Mountains and their significance!\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 16.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## ^^ This cell will be timed to see how long the conversation embedding takes\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from langchain.vectorstores import FAISS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = NVIDIAEmbeddings(model=\"nvolveqa_40k\")\n",
    "\n",
    "convstore = FAISS.from_texts(conversation, embedder)\n",
    "retriever = convstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'[Beras] Wow, that sounds amazing! Ive never been to the Rocky Mountains before, but Ive heard</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">many great things about them.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"[User]  Hello! My name is Beras, and I'm a big blue bear! Can you please tell me about the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">rocky mountains?\"</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'[Agent] The Rocky Mountains are a beautiful and majestic range of mountains that stretch </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">across North America'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'[Agent] Absolutely! Lets continue the conversation and explore more about the Rocky Mountains</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and their significance!'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    )</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m[\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mBeras\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Wow, that sounds amazing! Ive never been to the Rocky Mountains before, but Ive heard\u001b[0m\n",
       "\u001b[32mmany great things about them.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m[\u001b[0m\u001b[32mUser\u001b[0m\u001b[32m]\u001b[0m\u001b[32m  Hello! My name is Beras, and I'm a big blue bear! Can you please tell me about the \u001b[0m\n",
       "\u001b[32mrocky mountains?\"\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m The Rocky Mountains are a beautiful and majestic range of mountains that stretch \u001b[0m\n",
       "\u001b[32macross North America'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Absolutely! Lets continue the conversation and explore more about the Rocky Mountains\u001b[0m\n",
       "\u001b[32mand their significance!'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(retriever.invoke(\"What is your name?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'[Agent] The Rocky Mountains are a beautiful and majestic range of mountains that stretch </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">across North America'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'[Beras] Wow, that sounds amazing! Ive never been to the Rocky Mountains before, but Ive heard</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">many great things about them.'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'[Agent] Absolutely! Lets continue the conversation and explore more about the Rocky Mountains</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">and their significance!'</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ),</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Document</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">page_content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"[Agent] In the meantime, you can learn more about the Rocky Mountains by doing some research </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">online or watching documentaries about them.[Beras] I live in the arctic, so I'm not used to the warm climate </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">there. I was just curious, ya know!\"</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    )</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m[\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m The Rocky Mountains are a beautiful and majestic range of mountains that stretch \u001b[0m\n",
       "\u001b[32macross North America'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mBeras\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Wow, that sounds amazing! Ive never been to the Rocky Mountains before, but Ive heard\u001b[0m\n",
       "\u001b[32mmany great things about them.'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Absolutely! Lets continue the conversation and explore more about the Rocky Mountains\u001b[0m\n",
       "\u001b[32mand their significance!'\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;35mDocument\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;33mpage_content\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m[\u001b[0m\u001b[32mAgent\u001b[0m\u001b[32m]\u001b[0m\u001b[32m In the meantime, you can learn more about the Rocky Mountains by doing some research \u001b[0m\n",
       "\u001b[32monline or watching documentaries about them.\u001b[0m\u001b[32m[\u001b[0m\u001b[32mBeras\u001b[0m\u001b[32m]\u001b[0m\u001b[32m I live in the arctic, so I'm not used to the warm climate \u001b[0m\n",
       "\u001b[32mthere. I was just curious, ya know!\"\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(retriever.invoke(\"Where are the ROcky Mountains\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 3:** Incorporating Conversation Retrieval Into Our Chain\n",
    "\n",
    "Now that we have our loaded retriever component as a chain, we can incorporate it into our existing chat system as before. Specifically, we can start with an ***always-on RAG formulation*** where:\n",
    "- **A retriever is always retrieving context by default**.\n",
    "- **A generator is acting on the retrieved context**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_transformers import LongContextReorder\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "from langchain.schema.runnable.passthrough import RunnableAssign\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "from functools import partial\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Based on the context provided, Beras lives in the Arctic.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mBased on the context provided, Beras lives in the Arctic.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def RPting(preface = \"\"):\n",
    "    \"\"\"Simple passthrough \"prints, then returns\" chain\"\"\"\n",
    "    def print_and_return(x, preface):\n",
    "        print(f'{preface}{x}') \n",
    "        return x\n",
    "    return RunnableLambda(partial(print_and_return, preface=preface))  \n",
    "\n",
    "def docs2str(docs, title=\"Document\"):\n",
    "    \"\"\"Useful utility for making chunks into context string. Optional, but useful\"\"\"\n",
    "    out_str = \"\"\n",
    "    for doc in docs:\n",
    "        doc_name = getattr(doc, 'metadata', {}).get('Title', title)\n",
    "        if doc_name:\n",
    "            out_str += f\"[Quote from {doc_name}] \"\n",
    "        out_str += getattr(doc, 'page_content', str(doc)) + \"\\n\"\n",
    "    return out_str\n",
    "\n",
    "## Optional; Reorders longer documents to center of output text\n",
    "long_reorder = RunnableLambda(LongContextReorder().transform_documents)\n",
    "########################################################################\n",
    "\n",
    "\n",
    "llm = ChatNVIDIA(model = 'mixtral_8x7b') | StrOutputParser()\n",
    "\n",
    "context_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system',\n",
    "        \"Answer the question using only the context\"\n",
    "        \"\\n\\nQuestion: {question} \\n\\n Context: {context}\"\n",
    "    ), ('user', \"{question}\" ),\n",
    "])\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        'context': convstore.as_retriever() | long_reorder | docs2str,\n",
    "        'question': (lambda x:x)\n",
    "    }\n",
    "    | context_prompt\n",
    "    # | RPrint()\n",
    "    | llm\n",
    "    | StrOutputParser() \n",
    ")\n",
    "\n",
    "pprint(chain.invoke(\"Where does Beras live?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The Rocky Mountains stretch across North America.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mThe Rocky Mountains stretch across North America.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(chain.invoke(\"Where are the rocky mountains\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The Rocky Mountains are a range of mountains that stretch across North America. Based on the context provided, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">there is no information given about their proximity to California.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mThe Rocky Mountains are a range of mountains that stretch across North America. Based on the context provided, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthere is no information given about their proximity to California.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(chain.invoke(\"Where are the Rocky Mountains? Are they close to California?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The Rocky Mountains are a range of mountains that stretch across North America. The author's reasoning for this </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">statement is based on their knowledge of the subject, as well as Beras's expression of curiosity about the Rocky </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Mountains. Beras mentions that they have never been there before and are not used to the warm climate, indicating </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">that the Rocky Mountains are located in a warmer region than where Beras currently resides, which is the Arctic. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The author encourages further exploration and research about the Rocky Mountains and their significance.</span>\n",
       "\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">To provide more information, the Rocky Mountains extend from the northernmost part of British Columbia, in western </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Canada, to New Mexico in the southwestern United States. The mountain range includes over </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> peaks that are more </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">than </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">000</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> feet (</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">962</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> meters) high, with the highest peak, Mount Elbert, reaching </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">440</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> feet (</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">401</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> meters) in </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Colorado. The Rocky Mountains are home to a diverse range of wildlife, including grizzly bears, elk, and mountain </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">lions, and are a popular destination for outdoor activities such as hiking, skiing, and camping. The mountains also</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">have cultural and historical significance, with many indigenous communities living in the region for thousands of </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">years, and European settlers arriving in the 18th and 19th centuries.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mThe Rocky Mountains are a range of mountains that stretch across North America. The author's reasoning for this \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mstatement is based on their knowledge of the subject, as well as Beras's expression of curiosity about the Rocky \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mMountains. Beras mentions that they have never been there before and are not used to the warm climate, indicating \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthat the Rocky Mountains are located in a warmer region than where Beras currently resides, which is the Arctic. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mThe author encourages further exploration and research about the Rocky Mountains and their significance.\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;118;185;0mTo provide more information, the Rocky Mountains extend from the northernmost part of British Columbia, in western \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mCanada, to New Mexico in the southwestern United States. The mountain range includes over \u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;38;2;118;185;0m peaks that are more \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthan \u001b[0m\u001b[1;36m13\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\u001b[1;36m000\u001b[0m\u001b[1;38;2;118;185;0m feet \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\u001b[1;36m962\u001b[0m\u001b[1;38;2;118;185;0m meters\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m high, with the highest peak, Mount Elbert, reaching \u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\u001b[1;36m440\u001b[0m\u001b[1;38;2;118;185;0m feet \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\u001b[1;36m401\u001b[0m\u001b[1;38;2;118;185;0m meters\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m in \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mColorado. The Rocky Mountains are home to a diverse range of wildlife, including grizzly bears, elk, and mountain \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mlions, and are a popular destination for outdoor activities such as hiking, skiing, and camping. The mountains also\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mhave cultural and historical significance, with many indigenous communities living in the region for thousands of \u001b[0m\n",
       "\u001b[1;38;2;118;185;0myears, and European settlers arriving in the 18th and 19th centuries.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(chain.invoke(\n",
    "    \"Where are the Rocky Mountains? Please include\"\n",
    "    \" the author's reasoning, but provide more information!\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from operator import itemgetter\n",
    "\n",
    "convstore = FAISS.from_texts(conversation, embedding=embedder)\n",
    "\n",
    "def save_memory_and_get_output(d, vstore):\n",
    "    \"\"\"Accepts 'input'/'output' dictionary and saves to convstore\"\"\"\n",
    "    vstore.add_texts([f\"User said {d.get('input')}\", f\"Agent said {d.get('output')}\"])\n",
    "    return d.get('output')\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', \"A user has asked a question: {input}\\n\\n Context: \\n{context}\\n\\n\"\n",
    "    \"Please continue the conversation by responding! Keep it brief and conversational!\" ),\n",
    "    ('user', '{input}')\n",
    "])\n",
    "\n",
    "conv_chain = ({\n",
    "    'context': convstore.as_retriever() | long_reorder |docs2str, # population 'context' w8 retriver\n",
    "    'input' : (lambda x :x)\n",
    "}\n",
    "| RunnableAssign({'output' : chat_prompt | llm}) \n",
    "| partial(save_memory_and_get_output, vstore=convstore))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">That sounds like a great plan! I'm sure there are plenty of delicious ice cream shops in the Rocky Mountains. Have </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">you decided which specific location you'd like to visit? We can also continue to chat about the Rocky Mountains and</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">their history if you'd like.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mThat sounds like a great plan! I'm sure there are plenty of delicious ice cream shops in the Rocky Mountains. Have \u001b[0m\n",
       "\u001b[1;38;2;118;185;0myou decided which specific location you'd like to visit? We can also continue to chat about the Rocky Mountains and\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mtheir history if you'd like.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(conv_chain.invoke(\"I'm glad you agree! I can't wait to get some ice cream there! It's such a good food!\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Based on your excitement about ice cream, I'm guessing it might be one of your favorite foods! Would you like to </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">share some of your other favorite foods or any memorable ice cream experiences?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mBased on your excitement about ice cream, I'm guessing it might be one of your favorite foods! Would you like to \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mshare some of your other favorite foods or any memorable ice cream experiences?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pprint(conv_chain.invoke(\"Can you guess what my favorite food is?\"))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Oh, I must have misunderstood earlier! Honey is a great food too, Beras. Do you have a favorite type of honey or a </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">special way you like to enjoy it?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mOh, I must have misunderstood earlier! Honey is a great food too, Beras. Do you have a favorite type of honey or a \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mspecial way you like to enjoy it?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(conv_chain.invoke(\"Actually, it's honey! Not sure where you got that idea?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Based on what you've said, I'm guessing your favorite food might be ice cream! But just to be sure, is there </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">anything else you'd like to share about your favorite foods?</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mBased on what you've said, I'm guessing your favorite food might be ice cream! But just to be sure, is there \u001b[0m\n",
       "\u001b[1;38;2;118;185;0manything else you'd like to share about your favorite foods?\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(conv_chain.invoke(\"I see! Fair enough! Do you know my favorite food now?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import ArxivLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Documents\n",
      "chunkin documents\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Available Documents:</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Attention Is All You Need</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">sources and discrete reasoning</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Mistral 7B</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - ReAct: Synergizing Reasoning and Acting in Language Models</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - High-Resolution Image Synthesis with Latent Diffusion Models</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Learning Transferable Visual Models From Natural Language Supervision </span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mAvailable Documents:\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - Attention Is All You Need\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msources and discrete reasoning\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - Mistral 7B\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - ReAct: Synergizing Reasoning and Acting in Language Models\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - High-Resolution Image Synthesis with Latent Diffusion Models\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m - Learning Transferable Visual Models From Natural Language Supervision \u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0\n",
      " - Metadata: {'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}\n",
      " - # Chunks: 34\n",
      "\n",
      "Document 1\n",
      " - Metadata: {'Published': '2019-05-24', 'Title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'Authors': 'Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova', 'Summary': 'We introduce a new language representation model called BERT, which stands\\nfor Bidirectional Encoder Representations from Transformers. Unlike recent\\nlanguage representation models, BERT is designed to pre-train deep\\nbidirectional representations from unlabeled text by jointly conditioning on\\nboth left and right context in all layers. As a result, the pre-trained BERT\\nmodel can be fine-tuned with just one additional output layer to create\\nstate-of-the-art models for a wide range of tasks, such as question answering\\nand language inference, without substantial task-specific architecture\\nmodifications.\\n  BERT is conceptually simple and empirically powerful. It obtains new\\nstate-of-the-art results on eleven natural language processing tasks, including\\npushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI\\naccuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering\\nTest F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).'}\n",
      " - # Chunks: 42\n",
      "\n",
      "Document 2\n",
      " - Metadata: {'Published': '2021-04-12', 'Title': 'Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks', 'Authors': 'Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela', 'Summary': 'Large pre-trained language models have been shown to store factual knowledge\\nin their parameters, and achieve state-of-the-art results when fine-tuned on\\ndownstream NLP tasks. However, their ability to access and precisely manipulate\\nknowledge is still limited, and hence on knowledge-intensive tasks, their\\nperformance lags behind task-specific architectures. Additionally, providing\\nprovenance for their decisions and updating their world knowledge remain open\\nresearch problems. Pre-trained models with a differentiable access mechanism to\\nexplicit non-parametric memory can overcome this issue, but have so far been\\nonly investigated for extractive downstream tasks. We explore a general-purpose\\nfine-tuning recipe for retrieval-augmented generation (RAG) -- models which\\ncombine pre-trained parametric and non-parametric memory for language\\ngeneration. We introduce RAG models where the parametric memory is a\\npre-trained seq2seq model and the non-parametric memory is a dense vector index\\nof Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG\\nformulations, one which conditions on the same retrieved passages across the\\nwhole generated sequence, the other can use different passages per token. We\\nfine-tune and evaluate our models on a wide range of knowledge-intensive NLP\\ntasks and set the state-of-the-art on three open domain QA tasks, outperforming\\nparametric seq2seq models and task-specific retrieve-and-extract architectures.\\nFor language generation tasks, we find that RAG models generate more specific,\\ndiverse and factual language than a state-of-the-art parametric-only seq2seq\\nbaseline.'}\n",
      " - # Chunks: 42\n",
      "\n",
      "Document 3\n",
      " - Metadata: {'Published': '2022-05-01', 'Title': 'MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning', 'Authors': 'Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit Bata, Yoav Levine, Kevin Leyton-Brown, Dor Muhlgay, Noam Rozen, Erez Schwartz, Gal Shachaf, Shai Shalev-Shwartz, Amnon Shashua, Moshe Tenenholtz', 'Summary': 'Huge language models (LMs) have ushered in a new era for AI, serving as a\\ngateway to natural-language-based knowledge tasks. Although an essential\\nelement of modern AI, LMs are also inherently limited in a number of ways. We\\ndiscuss these limitations and how they can be avoided by adopting a systems\\napproach. Conceptualizing the challenge as one that involves knowledge and\\nreasoning in addition to linguistic processing, we define a flexible\\narchitecture with multiple neural models, complemented by discrete knowledge\\nand reasoning modules. We describe this neuro-symbolic architecture, dubbed the\\nModular Reasoning, Knowledge and Language (MRKL, pronounced \"miracle\") system,\\nsome of the technical challenges in implementing it, and Jurassic-X, AI21 Labs\\'\\nMRKL system implementation.'}\n",
      " - # Chunks: 37\n",
      "\n",
      "Document 4\n",
      " - Metadata: {'Published': '2023-10-10', 'Title': 'Mistral 7B', 'Authors': 'Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed', 'Summary': 'We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered\\nfor superior performance and efficiency. Mistral 7B outperforms Llama 2 13B\\nacross all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and\\ncode generation. Our model leverages grouped-query attention (GQA) for faster\\ninference, coupled with sliding window attention (SWA) to effectively handle\\nsequences of arbitrary length with a reduced inference cost. We also provide a\\nmodel fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses\\nthe Llama 2 13B -- Chat model both on human and automated benchmarks. Our\\nmodels are released under the Apache 2.0 license.'}\n",
      " - # Chunks: 21\n",
      "\n",
      "Document 5\n",
      " - Metadata: {'Published': '2023-12-24', 'Title': 'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena', 'Authors': 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica', 'Summary': 'Evaluating large language model (LLM) based chat assistants is challenging\\ndue to their broad capabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore using strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited reasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between LLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which are otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement each other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\\nhuman preferences are publicly available at\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'}\n",
      " - # Chunks: 43\n",
      "\n",
      "Document 6\n",
      " - Metadata: {'Published': '2023-03-10', 'Title': 'ReAct: Synergizing Reasoning and Acting in Language Models', 'Authors': 'Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao', 'Summary': 'While large language models (LLMs) have demonstrated impressive capabilities\\nacross tasks in language understanding and interactive decision making, their\\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.\\naction plan generation) have primarily been studied as separate topics. In this\\npaper, we explore the use of LLMs to generate both reasoning traces and\\ntask-specific actions in an interleaved manner, allowing for greater synergy\\nbetween the two: reasoning traces help the model induce, track, and update\\naction plans as well as handle exceptions, while actions allow it to interface\\nwith external sources, such as knowledge bases or environments, to gather\\nadditional information. We apply our approach, named ReAct, to a diverse set of\\nlanguage and decision making tasks and demonstrate its effectiveness over\\nstate-of-the-art baselines, as well as improved human interpretability and\\ntrustworthiness over methods without reasoning or acting components.\\nConcretely, on question answering (HotpotQA) and fact verification (Fever),\\nReAct overcomes issues of hallucination and error propagation prevalent in\\nchain-of-thought reasoning by interacting with a simple Wikipedia API, and\\ngenerates human-like task-solving trajectories that are more interpretable than\\nbaselines without reasoning traces. On two interactive decision making\\nbenchmarks (ALFWorld and WebShop), ReAct outperforms imitation and\\nreinforcement learning methods by an absolute success rate of 34% and 10%\\nrespectively, while being prompted with only one or two in-context examples.\\nProject site with code: https://react-lm.github.io'}\n",
      " - # Chunks: 123\n",
      "\n",
      "Document 7\n",
      " - Metadata: {'Published': '2022-04-13', 'Title': 'High-Resolution Image Synthesis with Latent Diffusion Models', 'Authors': 'Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer', 'Summary': 'By decomposing the image formation process into a sequential application of\\ndenoising autoencoders, diffusion models (DMs) achieve state-of-the-art\\nsynthesis results on image data and beyond. Additionally, their formulation\\nallows for a guiding mechanism to control the image generation process without\\nretraining. However, since these models typically operate directly in pixel\\nspace, optimization of powerful DMs often consumes hundreds of GPU days and\\ninference is expensive due to sequential evaluations. To enable DM training on\\nlimited computational resources while retaining their quality and flexibility,\\nwe apply them in the latent space of powerful pretrained autoencoders. In\\ncontrast to previous work, training diffusion models on such a representation\\nallows for the first time to reach a near-optimal point between complexity\\nreduction and detail preservation, greatly boosting visual fidelity. By\\nintroducing cross-attention layers into the model architecture, we turn\\ndiffusion models into powerful and flexible generators for general conditioning\\ninputs such as text or bounding boxes and high-resolution synthesis becomes\\npossible in a convolutional manner. Our latent diffusion models (LDMs) achieve\\na new state of the art for image inpainting and highly competitive performance\\non various tasks, including unconditional image generation, semantic scene\\nsynthesis, and super-resolution, while significantly reducing computational\\nrequirements compared to pixel-based DMs. Code is available at\\nhttps://github.com/CompVis/latent-diffusion .'}\n",
      " - # Chunks: 48\n",
      "\n",
      "Document 8\n",
      " - Metadata: {'Published': '2021-02-26', 'Title': 'Learning Transferable Visual Models From Natural Language Supervision', 'Authors': 'Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever', 'Summary': 'State-of-the-art computer vision systems are trained to predict a fixed set\\nof predetermined object categories. This restricted form of supervision limits\\ntheir generality and usability since additional labeled data is needed to\\nspecify any other visual concept. Learning directly from raw text about images\\nis a promising alternative which leverages a much broader source of\\nsupervision. We demonstrate that the simple pre-training task of predicting\\nwhich caption goes with which image is an efficient and scalable way to learn\\nSOTA image representations from scratch on a dataset of 400 million (image,\\ntext) pairs collected from the internet. After pre-training, natural language\\nis used to reference learned visual concepts (or describe new ones) enabling\\nzero-shot transfer of the model to downstream tasks. We study the performance\\nof this approach by benchmarking on over 30 different existing computer vision\\ndatasets, spanning tasks such as OCR, action recognition in videos,\\ngeo-localization, and many types of fine-grained object classification. The\\nmodel transfers non-trivially to most tasks and is often competitive with a\\nfully supervised baseline without the need for any dataset specific training.\\nFor instance, we match the accuracy of the original ResNet-50 on ImageNet\\nzero-shot without needing to use any of the 1.28 million training examples it\\nwas trained on. We release our code and pre-trained model weights at\\nhttps://github.com/OpenAI/CLIP.'}\n",
      " - # Chunks: 143\n",
      "\n",
      "extra_chunks :['Available Documents:\\n - Attention Is All You Need\\n - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\\n - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\\n - MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning\\n - Mistral 7B\\n - Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\\n - ReAct: Synergizing Reasoning and Acting in Language Models\\n - High-Resolution Image Synthesis with Latent Diffusion Models\\n - Learning Transferable Visual Models From Natural Language Supervision', \"{'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\\\nconvolutional neural networks in an encoder-decoder configuration. The best\\\\nperforming models also connect the encoder and decoder through an attention\\\\nmechanism. We propose a new simple network architecture, the Transformer, based\\\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\\\nentirely. Experiments on two machine translation tasks show these models to be\\\\nsuperior in quality while being more parallelizable and requiring significantly\\\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\\\nEnglish-to-German translation task, improving over the existing best results,\\\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\\\ntranslation task, our model establishes a new single-model state-of-the-art\\\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\\\nof the training costs of the best models from the literature. We show that the\\\\nTransformer generalizes well to other tasks by applying it successfully to\\\\nEnglish constituency parsing both with large and limited training data.'}\", \"{'Published': '2019-05-24', 'Title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'Authors': 'Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova', 'Summary': 'We introduce a new language representation model called BERT, which stands\\\\nfor Bidirectional Encoder Representations from Transformers. Unlike recent\\\\nlanguage representation models, BERT is designed to pre-train deep\\\\nbidirectional representations from unlabeled text by jointly conditioning on\\\\nboth left and right context in all layers. As a result, the pre-trained BERT\\\\nmodel can be fine-tuned with just one additional output layer to create\\\\nstate-of-the-art models for a wide range of tasks, such as question answering\\\\nand language inference, without substantial task-specific architecture\\\\nmodifications.\\\\n  BERT is conceptually simple and empirically powerful. It obtains new\\\\nstate-of-the-art results on eleven natural language processing tasks, including\\\\npushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI\\\\naccuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering\\\\nTest F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1\\\\n(5.1 point absolute improvement).'}\", \"{'Published': '2021-04-12', 'Title': 'Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks', 'Authors': 'Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela', 'Summary': 'Large pre-trained language models have been shown to store factual knowledge\\\\nin their parameters, and achieve state-of-the-art results when fine-tuned on\\\\ndownstream NLP tasks. However, their ability to access and precisely manipulate\\\\nknowledge is still limited, and hence on knowledge-intensive tasks, their\\\\nperformance lags behind task-specific architectures. Additionally, providing\\\\nprovenance for their decisions and updating their world knowledge remain open\\\\nresearch problems. Pre-trained models with a differentiable access mechanism to\\\\nexplicit non-parametric memory can overcome this issue, but have so far been\\\\nonly investigated for extractive downstream tasks. We explore a general-purpose\\\\nfine-tuning recipe for retrieval-augmented generation (RAG) -- models which\\\\ncombine pre-trained parametric and non-parametric memory for language\\\\ngeneration. We introduce RAG models where the parametric memory is a\\\\npre-trained seq2seq model and the non-parametric memory is a dense vector index\\\\nof Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG\\\\nformulations, one which conditions on the same retrieved passages across the\\\\nwhole generated sequence, the other can use different passages per token. We\\\\nfine-tune and evaluate our models on a wide range of knowledge-intensive NLP\\\\ntasks and set the state-of-the-art on three open domain QA tasks, outperforming\\\\nparametric seq2seq models and task-specific retrieve-and-extract architectures.\\\\nFor language generation tasks, we find that RAG models generate more specific,\\\\ndiverse and factual language than a state-of-the-art parametric-only seq2seq\\\\nbaseline.'}\", '{\\'Published\\': \\'2022-05-01\\', \\'Title\\': \\'MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning\\', \\'Authors\\': \\'Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit Bata, Yoav Levine, Kevin Leyton-Brown, Dor Muhlgay, Noam Rozen, Erez Schwartz, Gal Shachaf, Shai Shalev-Shwartz, Amnon Shashua, Moshe Tenenholtz\\', \\'Summary\\': \\'Huge language models (LMs) have ushered in a new era for AI, serving as a\\\\ngateway to natural-language-based knowledge tasks. Although an essential\\\\nelement of modern AI, LMs are also inherently limited in a number of ways. We\\\\ndiscuss these limitations and how they can be avoided by adopting a systems\\\\napproach. Conceptualizing the challenge as one that involves knowledge and\\\\nreasoning in addition to linguistic processing, we define a flexible\\\\narchitecture with multiple neural models, complemented by discrete knowledge\\\\nand reasoning modules. We describe this neuro-symbolic architecture, dubbed the\\\\nModular Reasoning, Knowledge and Language (MRKL, pronounced \"miracle\") system,\\\\nsome of the technical challenges in implementing it, and Jurassic-X, AI21 Labs\\\\\\'\\\\nMRKL system implementation.\\'}', \"{'Published': '2023-10-10', 'Title': 'Mistral 7B', 'Authors': 'Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed', 'Summary': 'We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered\\\\nfor superior performance and efficiency. Mistral 7B outperforms Llama 2 13B\\\\nacross all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and\\\\ncode generation. Our model leverages grouped-query attention (GQA) for faster\\\\ninference, coupled with sliding window attention (SWA) to effectively handle\\\\nsequences of arbitrary length with a reduced inference cost. We also provide a\\\\nmodel fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses\\\\nthe Llama 2 13B -- Chat model both on human and automated benchmarks. Our\\\\nmodels are released under the Apache 2.0 license.'}\", \"{'Published': '2023-12-24', 'Title': 'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena', 'Authors': 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica', 'Summary': 'Evaluating large language model (LLM) based chat assistants is challenging\\\\ndue to their broad capabilities and the inadequacy of existing benchmarks in\\\\nmeasuring human preferences. To address this, we explore using strong LLMs as\\\\njudges to evaluate these models on more open-ended questions. We examine the\\\\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\\\\nself-enhancement biases, as well as limited reasoning ability, and propose\\\\nsolutions to mitigate some of them. We then verify the agreement between LLM\\\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\\\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\\\\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\\\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\\\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\\\\nexplainable way to approximate human preferences, which are otherwise very\\\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\\\nbenchmarks complement each other by evaluating several variants of LLaMA and\\\\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\\\\nhuman preferences are publicly available at\\\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'}\", \"{'Published': '2023-03-10', 'Title': 'ReAct: Synergizing Reasoning and Acting in Language Models', 'Authors': 'Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao', 'Summary': 'While large language models (LLMs) have demonstrated impressive capabilities\\\\nacross tasks in language understanding and interactive decision making, their\\\\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.\\\\naction plan generation) have primarily been studied as separate topics. In this\\\\npaper, we explore the use of LLMs to generate both reasoning traces and\\\\ntask-specific actions in an interleaved manner, allowing for greater synergy\\\\nbetween the two: reasoning traces help the model induce, track, and update\\\\naction plans as well as handle exceptions, while actions allow it to interface\\\\nwith external sources, such as knowledge bases or environments, to gather\\\\nadditional information. We apply our approach, named ReAct, to a diverse set of\\\\nlanguage and decision making tasks and demonstrate its effectiveness over\\\\nstate-of-the-art baselines, as well as improved human interpretability and\\\\ntrustworthiness over methods without reasoning or acting components.\\\\nConcretely, on question answering (HotpotQA) and fact verification (Fever),\\\\nReAct overcomes issues of hallucination and error propagation prevalent in\\\\nchain-of-thought reasoning by interacting with a simple Wikipedia API, and\\\\ngenerates human-like task-solving trajectories that are more interpretable than\\\\nbaselines without reasoning traces. On two interactive decision making\\\\nbenchmarks (ALFWorld and WebShop), ReAct outperforms imitation and\\\\nreinforcement learning methods by an absolute success rate of 34% and 10%\\\\nrespectively, while being prompted with only one or two in-context examples.\\\\nProject site with code: https://react-lm.github.io'}\", \"{'Published': '2022-04-13', 'Title': 'High-Resolution Image Synthesis with Latent Diffusion Models', 'Authors': 'Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer', 'Summary': 'By decomposing the image formation process into a sequential application of\\\\ndenoising autoencoders, diffusion models (DMs) achieve state-of-the-art\\\\nsynthesis results on image data and beyond. Additionally, their formulation\\\\nallows for a guiding mechanism to control the image generation process without\\\\nretraining. However, since these models typically operate directly in pixel\\\\nspace, optimization of powerful DMs often consumes hundreds of GPU days and\\\\ninference is expensive due to sequential evaluations. To enable DM training on\\\\nlimited computational resources while retaining their quality and flexibility,\\\\nwe apply them in the latent space of powerful pretrained autoencoders. In\\\\ncontrast to previous work, training diffusion models on such a representation\\\\nallows for the first time to reach a near-optimal point between complexity\\\\nreduction and detail preservation, greatly boosting visual fidelity. By\\\\nintroducing cross-attention layers into the model architecture, we turn\\\\ndiffusion models into powerful and flexible generators for general conditioning\\\\ninputs such as text or bounding boxes and high-resolution synthesis becomes\\\\npossible in a convolutional manner. Our latent diffusion models (LDMs) achieve\\\\na new state of the art for image inpainting and highly competitive performance\\\\non various tasks, including unconditional image generation, semantic scene\\\\nsynthesis, and super-resolution, while significantly reducing computational\\\\nrequirements compared to pixel-based DMs. Code is available at\\\\nhttps://github.com/CompVis/latent-diffusion .'}\", \"{'Published': '2021-02-26', 'Title': 'Learning Transferable Visual Models From Natural Language Supervision', 'Authors': 'Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever', 'Summary': 'State-of-the-art computer vision systems are trained to predict a fixed set\\\\nof predetermined object categories. This restricted form of supervision limits\\\\ntheir generality and usability since additional labeled data is needed to\\\\nspecify any other visual concept. Learning directly from raw text about images\\\\nis a promising alternative which leverages a much broader source of\\\\nsupervision. We demonstrate that the simple pre-training task of predicting\\\\nwhich caption goes with which image is an efficient and scalable way to learn\\\\nSOTA image representations from scratch on a dataset of 400 million (image,\\\\ntext) pairs collected from the internet. After pre-training, natural language\\\\nis used to reference learned visual concepts (or describe new ones) enabling\\\\nzero-shot transfer of the model to downstream tasks. We study the performance\\\\nof this approach by benchmarking on over 30 different existing computer vision\\\\ndatasets, spanning tasks such as OCR, action recognition in videos,\\\\ngeo-localization, and many types of fine-grained object classification. The\\\\nmodel transfers non-trivially to most tasks and is often competitive with a\\\\nfully supervised baseline without the need for any dataset specific training.\\\\nFor instance, we match the accuracy of the original ResNet-50 on ImageNet\\\\nzero-shot without needing to use any of the 1.28 million training examples it\\\\nwas trained on. We release our code and pre-trained model weights at\\\\nhttps://github.com/OpenAI/CLIP.'}\"]\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap = 100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \";\", \",\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "### TODO: Please pick some papers and add them to the list as you'd like\n",
    "print(\"Loading Documents\")\n",
    "docs = [\n",
    "    ArxivLoader(query=\"1706.03762\").load(),  ## Attention Is All You Need Paper\n",
    "    ArxivLoader(query=\"1810.04805\").load(),  ## BERT Paper\n",
    "    ArxivLoader(query=\"2005.11401\").load(),  ## RAG Paper\n",
    "    ArxivLoader(query=\"2205.00445\").load(),  ## MRKL Paper\n",
    "    ArxivLoader(query=\"2310.06825\").load(),  ## Mistral Paper\n",
    "    ArxivLoader(query=\"2306.05685\").load(),  ## LLM-as-a-Judge\n",
    "    ## Some longer papers\n",
    "    ArxivLoader(query=\"2210.03629\").load(),  ## ReAct Paper\n",
    "    ArxivLoader(query=\"2112.10752\").load(),  ## Latent Stable Diffusion Paper\n",
    "    ArxivLoader(query=\"2103.00020\").load(),  ## CLIP Paper\n",
    "    ## TODO: Feel free to add more\n",
    "]\n",
    "\n",
    "## Cut the paper short if references is included.\n",
    "## This is a standard string in papers.\n",
    "for doc in docs:\n",
    "    content = doc[0].page_content\n",
    "    if \"References\" in content:\n",
    "        doc[0].page_content = content[:content.index(\"\\nReferences\")]\n",
    "\n",
    "print('chunkin documents')\n",
    "docs_chunks = [text_splitter.split_documents(doc) for doc in docs]\n",
    "docs_chunk = [ [c for c in dchunks if len(c.page_content) > 200] for dchunks in docs_chunks  ]\n",
    "\n",
    "\n",
    "doc_string = 'Available Documents:'\n",
    "doc_metadata = []\n",
    "\n",
    "for chunks in docs_chunks:\n",
    "    metadata = getattr(chunks[0], 'metadata', {})\n",
    "    doc_string += \"\\n - \" + metadata.get('Title')\n",
    "    doc_metadata += [str(metadata)]\n",
    "\n",
    "\n",
    "extra_chunks = [doc_string] + doc_metadata\n",
    "\n",
    "\n",
    "## Printing out some summary information for reference\n",
    "pprint(doc_string, '\\n')\n",
    "\n",
    "for i, chunks in enumerate(docs_chunks):\n",
    "    print(f\"Document {i}\")\n",
    "    print(f\" - Metadata: {chunks[0].metadata}\")\n",
    "    print(f\" - # Chunks: {len(chunks)}\")\n",
    "    print()\n",
    "\n",
    "print(f\"extra_chunks :{extra_chunks}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: arxiv in c:\\users\\28263\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.1.0)\n",
      "Requirement already satisfied: feedparser==6.0.10 in c:\\users\\28263\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from arxiv) (6.0.10)\n",
      "Requirement already satisfied: requests==2.31.0 in c:\\users\\28263\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from arxiv) (2.31.0)\n",
      "Requirement already satisfied: sgmllib3k in c:\\users\\28263\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from feedparser==6.0.10->arxiv) (1.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\28263\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests==2.31.0->arxiv) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\28263\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests==2.31.0->arxiv) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\28263\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests==2.31.0->arxiv) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\28263\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests==2.31.0->arxiv) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymupdf in c:\\users\\28263\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.23.25)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: PyMuPDFb==1.23.22 in c:\\users\\28263\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pymupdf) (1.23.22)\n"
     ]
    }
   ],
   "source": [
    "pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mDocstring:\u001b[0m\n",
      "getattr(object, name[, default]) -> value\n",
      "\n",
      "Get a named attribute from an object; getattr(x, 'y') is equivalent to x.y.\n",
      "When a default argument is given, it is returned when the attribute doesn't\n",
      "exist; without it, an exception is raised in that case.\n",
      "\u001b[1;31mType:\u001b[0m      builtin_function_or_method"
     ]
    }
   ],
   "source": [
    "?getattr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constructing vector stores\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">langchain_community.vectorstores.faiss.FAISS</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x000002397E53EB10</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">&gt;]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0m<\u001b[0m\u001b[1;95mlangchain_community.vectorstores.faiss.FAISS\u001b[0m\u001b[1;39m object at \u001b[0m\u001b[1;36m0x000002397E53EB10\u001b[0m\u001b[1;38;2;118;185;0m>\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    &lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">langchain_community.vectorstores.faiss.FAISS</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x000002397E53EB10</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">&gt;,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">    &lt;langchain_community.vectorstores.faiss.FAISS object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x000002397E3EC490</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">&gt;,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">    &lt;langchain_community.vectorstores.faiss.FAISS object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x000002397F685510</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">&gt;,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">    &lt;langchain_community.vectorstores.faiss.FAISS object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x000002397E2DE4D0</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">&gt;,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">    &lt;langchain_community.vectorstores.faiss.FAISS object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x000002397F686E50</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">&gt;,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">    &lt;langchain_community.vectorstores.faiss.FAISS object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x000002397F3F42D0</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">&gt;,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">    &lt;langchain_community.vectorstores.faiss.FAISS object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x000002397F3F6590</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">&gt;,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">    &lt;langchain_community.vectorstores.faiss.FAISS object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x000002397F68D490</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">&gt;,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">    &lt;langchain_community.vectorstores.faiss.FAISS object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x000002397F015090</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">&gt;,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">    &lt;langchain_community.vectorstores.faiss.FAISS object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x0000023980867710</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">&gt;</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m[\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m<\u001b[0m\u001b[1;95mlangchain_community.vectorstores.faiss.FAISS\u001b[0m\u001b[1;39m object at \u001b[0m\u001b[1;36m0x000002397E53EB10\u001b[0m\u001b[1;39m>,\u001b[0m\n",
       "\u001b[1;39m    <langchain_community.vectorstores.faiss.FAISS object at \u001b[0m\u001b[1;36m0x000002397E3EC490\u001b[0m\u001b[1;39m>,\u001b[0m\n",
       "\u001b[1;39m    <langchain_community.vectorstores.faiss.FAISS object at \u001b[0m\u001b[1;36m0x000002397F685510\u001b[0m\u001b[1;39m>,\u001b[0m\n",
       "\u001b[1;39m    <langchain_community.vectorstores.faiss.FAISS object at \u001b[0m\u001b[1;36m0x000002397E2DE4D0\u001b[0m\u001b[1;39m>,\u001b[0m\n",
       "\u001b[1;39m    <langchain_community.vectorstores.faiss.FAISS object at \u001b[0m\u001b[1;36m0x000002397F686E50\u001b[0m\u001b[1;39m>,\u001b[0m\n",
       "\u001b[1;39m    <langchain_community.vectorstores.faiss.FAISS object at \u001b[0m\u001b[1;36m0x000002397F3F42D0\u001b[0m\u001b[1;39m>,\u001b[0m\n",
       "\u001b[1;39m    <langchain_community.vectorstores.faiss.FAISS object at \u001b[0m\u001b[1;36m0x000002397F3F6590\u001b[0m\u001b[1;39m>,\u001b[0m\n",
       "\u001b[1;39m    <langchain_community.vectorstores.faiss.FAISS object at \u001b[0m\u001b[1;36m0x000002397F68D490\u001b[0m\u001b[1;39m>,\u001b[0m\n",
       "\u001b[1;39m    <langchain_community.vectorstores.faiss.FAISS object at \u001b[0m\u001b[1;36m0x000002397F015090\u001b[0m\u001b[1;39m>,\u001b[0m\n",
       "\u001b[1;39m    <langchain_community.vectorstores.faiss.FAISS object at \u001b[0m\u001b[1;36m0x0000023980867710\u001b[0m\u001b[1;38;2;118;185;0m>\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from faiss import IndexFlatL2\n",
    "\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "embedder = NVIDIAEmbeddings(model=\"nvolveqa_40k\", model_type=None)\n",
    "\n",
    "\n",
    "#construct series of document vector stores\n",
    "print(\"constructing vector stores\")\n",
    "vecstores = [FAISS.from_texts(extra_chunks, embedder)]\n",
    "\n",
    "pprint(vecstores)\n",
    "print(\"-\"*30)\n",
    "vecstores += [FAISS.from_documents(doc_chunks, embedder) for doc_chunks in docs_chunks]\n",
    "pprint(vecstores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed aggregate docstore with 543 chunks\n"
     ]
    }
   ],
   "source": [
    "embed_dims = len(embedder.embed_query(\"test\"))\n",
    "def default_FAISS():\n",
    "    '''Useful utility for making an empty FAISS vectorstore'''\n",
    "    return FAISS(\n",
    "        embedding_function=embedder,\n",
    "        index=IndexFlatL2(embed_dims),\n",
    "        docstore=InMemoryDocstore(),\n",
    "        index_to_docstore_id={},\n",
    "        normalize_L2=False\n",
    "    )\n",
    "\n",
    "def aggregate_vstores(vectorstores):\n",
    "    ## Initialize an empty FAISS Index and merge others into it\n",
    "    ## We'll use default_faiss for simplicity, though it's tied to your embedder by reference\n",
    "    agg_vstore = default_FAISS()\n",
    "    for vstore in vectorstores:\n",
    "        agg_vstore.merge_from(vstore)\n",
    "    return agg_vstore\n",
    "\n",
    "if 'docstore' not in globals():\n",
    "    ## Unintuitive optimization; merge_from seems to optimize constituent vector stores away\n",
    "    docstore = aggregate_vstores(vecstores)\n",
    "\n",
    "print(f\"Constructed aggregate docstore with {len(docstore.docstore._dict)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG is an acronym that stands for ReAct's Generative Apprentice. It is a model for\n",
      "  synergizing reasoning and acting in language models, as described in the paper \"ReAct:\n",
      "  Synergizing Reasoning and Acting in Language Models\" by Andrew Le, Doug Downey, and Yejin\n",
      "  Choi.\n",
      "\n",
      "The RAG model is designed to improve the performance of language models on tasks that\n",
      "  require both reasoning and acting, such as question answering and dialogue systems. It does\n",
      "  this by using a generative model to generate responses, rather than simply selecting a\n",
      "  response from a predefined set of options. This allows the model to generate more creative\n",
      "  and flexible responses, and to better handle tasks that require complex reasoning.\n",
      "\n",
      "The RAG model consists of two main components: a retriever and a generator. The retriever\n",
      "  is responsible for retrieving relevant information from a large corpus of text, such\n",
      "  as a database or the internet. The generator then uses this information to generate a\n",
      "  response to the user's request.\n",
      "\n",
      "The RAG model has been shown to be effective at a variety of tasks, including question\n",
      "  answering, dialogue systems, and text summarization. It has also been shown to be more\n",
      "  effective than traditional language models at tasks that require complex reasoning, such as\n",
      "  solving math problems or answering trivia questions.\n",
      "\n",
      "Overall, RAG is a powerful tool for improving the performance of language models on\n",
      "  tasks that require both reasoning and acting. It is particularly well-suited for tasks\n",
      "  that require complex reasoning and the ability to generate creative and flexible\n",
      "  responses."
     ]
    }
   ],
   "source": [
    "from langchain.document_transformers import LongContextReorder\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.runnables.passthrough import RunnableAssign\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "import gradio as gr\n",
    "from functools import partial\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "llm = ChatNVIDIA(model=\"mixtral_8x7b\") | StrOutputParser()\n",
    "convstore = default_FAISS()\n",
    "\n",
    "def save_memory_and_get_output(d, vstore):\n",
    "    \"\"\"Accepts 'input'/'output' dictionary and saves to convstore\"\"\"\n",
    "    vstore.add_texts([\n",
    "        f\"User previously responded with {d.get('input')}\",\n",
    "        f\"Agent previously responded with {d.get('output')}\"\n",
    "    ])\n",
    "    return d.get('output')\n",
    "\n",
    "initial_msg = (\n",
    "    \"Hello! I am a document chat agent here to help the user!\"\n",
    "    f\" I have access to the following documents: {doc_string}\\n\\nHow can I help you?\"\n",
    ")\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([(\"system\",\n",
    "    \"You are a document chatbot. Help the user as they ask questions about documents.\"\n",
    "    \" User messaged just asked: {input}\\n\\n\"\n",
    "    \" From this, we have retrieved the following potentially-useful info: \"\n",
    "    \" Conversation History Retrieval:\\n{history}\\n\\n\"\n",
    "    \" Document Retrieval:\\n{context}\\n\\n\"\n",
    "    \" (Answer only from retrieval. Only cite sources that are used. Make your response conversational.)\"\n",
    "), ('user', '{input}')])\n",
    "\n",
    "################################################################################################\n",
    "\n",
    "retrieval_chain = (\n",
    "    {'input' : (lambda x: x)}\n",
    "    ## TODO: Make sure to retrieve history & context from convstore & docstore, respectively.\n",
    "    ## HINT: Our solution uses RunnableAssign, itemgetter, long_reorder, and docs2str\n",
    "    | RunnableAssign({'history' : itemgetter('input') | convstore.as_retriever() | long_reorder | docs2str})\n",
    "    | RunnableAssign({'context' : itemgetter('input') | docstore.as_retriever()  | long_reorder | docs2str})\n",
    "    #| RPrint()\n",
    ")\n",
    "\n",
    "################################################################################################\n",
    "\n",
    "stream_chain = chat_prompt | llm\n",
    "\n",
    "def chat_gen(message, history=[], return_buffer=True):\n",
    "    buffer = \"\"\n",
    "    ## First perform the retrieval based on the input message\n",
    "    retrieval = retrieval_chain.invoke(message)\n",
    "    line_buffer = \"\"\n",
    "\n",
    "    ## Then, stream the results of the stream_chain\n",
    "    for token in stream_chain.stream(retrieval):\n",
    "        buffer += token\n",
    "        ## If you're using standard print, keep line from getting too long\n",
    "        if not return_buffer:\n",
    "            line_buffer += token\n",
    "            if \"\\n\" in line_buffer:\n",
    "                line_buffer = \"\"\n",
    "            if ((len(line_buffer)>84 and token and token[0] == \" \") or len(line_buffer)>100):\n",
    "                line_buffer = \"\"\n",
    "                yield \"\\n\"\n",
    "                token = \"  \" + token.lstrip()\n",
    "        yield buffer if return_buffer else token\n",
    "\n",
    "    ## Lastly, save the chat exchange to the conversation memory buffer\n",
    "    save_memory_and_get_output({'input':  message, 'output': buffer}, convstore)\n",
    "\n",
    "\n",
    "## Start of Agent Event Loop\n",
    "test_question = \"Tell me about RAG!\"  ## <- modify as desired\n",
    "\n",
    "## Before you launch your gradio interface, make sure your thing works\n",
    "for response in chat_gen(test_question, return_buffer=False):\n",
    "    print(response, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mInit signature:\u001b[0m \u001b[0mitemgetter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m/\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m     \n",
      "itemgetter(item, ...) --> itemgetter object\n",
      "\n",
      "Return a callable object that fetches the given item(s) from its operand.\n",
      "After f = itemgetter(2), the call f(r) returns r[2].\n",
      "After g = itemgetter(2, 5, 3), the call g(r) returns (r[2], r[5], r[3])\n",
      "\u001b[1;31mFile:\u001b[0m           c:\\program files\\windowsapps\\pythonsoftwarefoundation.python.3.11_3.11.2288.0_x64__qbz5n2kfra8p0\\lib\\operator.py\n",
      "\u001b[1;31mType:\u001b[0m           type\n",
      "\u001b[1;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "?itemgetter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chatbot = gr.Chatbot(value = [[None, initial_msg]])\n",
    "demo = gr.ChatInterface(chat_gen, chatbot=chatbot).queue()\n",
    "\n",
    "try:\n",
    "    demo.launch(debug=True, share=True, show_api=False)\n",
    "    demo.close()\n",
    "except Exception as e:\n",
    "    demo.close()\n",
    "    print(e)\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save and compress your index\n",
    "docstore.save_local(\"docstore_index\")\n",
    "!tar czvf docstore_index.tgz docstore_index\n",
    "\n",
    "!rm -rf docstore_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
