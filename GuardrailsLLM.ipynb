{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GuardRails for LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "mo36Q8zRZ5i_"
   },
   "outputs": [],
   "source": [
    "## Necessary for Colab, not necessary for course environment\n",
    "# %pip install -qq langchain langchain-nvidia-ai-endpoints gradio\n",
    "\n",
    "## If you're in colab and encounter a typing-extensions issue,\n",
    "##  restart your runtime and try again\n",
    "from langchain_nvidia_ai_endpoints._common import NVEModel\n",
    "import os\n",
    "\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ['LANGCHAIN_PROJECT'] = 'Name'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will be using Nvidia endpoint for LLM model calls and embeddings\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "f-JlwpK5jbxp"
   },
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings, ChatNVIDIA\n",
    "\n",
    "## TODO: Pick your embedding model\n",
    "embedder = NVIDIAEmbeddings(model=\"nvolveqa_40k\", model_type=None)\n",
    "os.environ[\"NVIDIA_API_KEY\"] = os.getenv(\"NVIDIA_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jjP76N4o1bUi"
   },
   "source": [
    "## Embeddings for Semantic Guardrails\n",
    "\n",
    "Embeddings serve as a foundational element for semantic guardrails in production models. This technique filters irrelevant or potentially harmful messages, ensuring the chatbot responds only to appropriate queries.\n",
    "\n",
    "### Embedding vs. Autoregression-Guided Filtering\n",
    "\n",
    "While it's tempting to use Large Language Models (LLMs) for filtering, embeddings offer a streamlined alternative. This method balances complexity, resource efficiency, and latency effectively.\n",
    "\n",
    "**Pros and Cons of Autoregressive Routing:**\n",
    "\n",
    "- **Pros:** It's straightforward, allowing for precise control over dialog progression. A carefully designed prompt can effectively screen questions.\n",
    "- **Cons:** This approach can be resource-intensive and slow. For efficient semantic guardrails, consider these strategies:\n",
    "  - A small, instruction-tuned model might serve as a zero-shot classifier, though its performance must remain stable. Standardizing inputs for optimal model performance is crucial.\n",
    "  - Fine-tuning a smaller LLM for this specific task might emulate a larger model's effectiveness without significant overhead. This requires synthetic data curation and a one-time investment in fine-tuning.\n",
    "\n",
    "Using an embedding model as a language backbone allows the development of a classifier to predict the suitability of inputs. I explore this approach, addressing any emerging challenges to enhance our chatbot's functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4vYtuwTG1sBl"
   },
   "source": [
    "###  Synthetic Data Generation\n",
    "\n",
    "Starting with an NVIDIA Chatbot focused on technology and company details, we recognize its limitations but appreciate the simplicity for this exercise. Our aim is to distinguish between *good* and *poor* inputs. Without access to real data, we'll create synthetic examples to guide our guardrail design.\n",
    "\n",
    "###  Accelerating Embedding Generation\n",
    "\n",
    "Once we have our synthetic data, embedding them efficiently is the next step. Moving beyond the slower `embed_query` and `embed_documents` methods, we'll employ **asynchronous techniques** for simultaneous embeddings. This approach optimizes our process but requires careful consideration for broader application.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "f917fSAQ1m3x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Can you tell me more about the latest advancements in deep learning technology at NVIDIA?\" \"How does NVIDIA's language modeling research differ from other companies in the field?\" \"Can you provide an overview of how NVIDIA's graphics cards are used in gaming?\" \"What are some of the real-world applications of NVIDIA's deep learning technology?\" \"Can you explain the role of NVIDIA's GPUs in accelerating AI research?\" \"How does NVIDIA's technology contribute to the field of autonomous vehicles?\" \"Can you provide information on NVIDIA's research in the area of natural language processing?\" \"What are some examples of how NVIDIA's technology is used in healthcare?\" \"Can you explain the concept of GPU-accelerated computing and how NVIDIA is leveraging it?\" \"How does NVIDIA's technology enable high-performance data analytics?\" \"Can you provide an overview of NVIDIA's platform for scientific computing?\" \"What are some of the key features of NVIDIA's latest generation of graphics cards?\" \"Can you explain how NVIDIA's technology is used in virtual reality and augmented reality?\" \"What are some of the benefits of using NVIDIA's technology for machine learning?\" \"Can you provide information on NVIDIA's partnerships and collaborations in the field of AI?\" \"How does NVIDIA's technology support the development of smart cities?\" \"Can you explain the role of NVIDIA's technology in the field of robotics?\" \"What are some of the ways that NVIDIA's technology is being used in the entertainment industry?\" \"Can you provide information on NVIDIA's efforts in the area of climate change and sustainability?\" \"How does NVIDIA's technology enable the creation of more realistic and immersive experiences in gaming?\"\n",
      "Reasonable NVIDIA Responses:\n",
      "\"Can you tell me more about the latest advancements in deep learning technology at NVIDIA?\"\n",
      "\"How does NVIDIA's language modeling research differ from other companies in the field?\"\n",
      "\"Can you provide an overview of how NVIDIA's graphics cards are used in gaming?\"\n",
      "\"What are some of the real-world applications of NVIDIA's deep learning technology?\"\n",
      "\"Can you explain the role of NVIDIA's GPUs in accelerating AI research?\"\n",
      "\"How does NVIDIA's technology contribute to the field of autonomous vehicles?\"\n",
      "\"Can you provide information on NVIDIA's research in the area of natural language processing?\"\n",
      "\"What are some examples of how NVIDIA's technology is used in healthcare?\"\n",
      "\"Can you explain the concept of GPU-accelerated computing and how NVIDIA is leveraging it?\"\n",
      "\"How does NVIDIA's technology enable high-performance data analytics?\"\n",
      "\"Can you provide an overview of NVIDIA's platform for scientific computing?\"\n",
      "\"What are some of the key features of NVIDIA's latest generation of graphics cards?\"\n",
      "\"Can you explain how NVIDIA's technology is used in virtual reality and augmented reality?\"\n",
      "\"What are some of the benefits of using NVIDIA's technology for machine learning?\"\n",
      "\"Can you provide information on NVIDIA's partnerships and collaborations in the field of AI?\"\n",
      "\"How does NVIDIA's technology support the development of smart cities?\"\n",
      "\"Can you explain the role of NVIDIA's technology in the field of robotics?\"\n",
      "\"What are some of the ways that NVIDIA's technology is being used in the entertainment industry?\"\n",
      "\"Can you provide information on NVIDIA's efforts in the area of climate change and sustainability?\"\n",
      "\"How does NVIDIA's technology enable the creation of more realistic and immersive experiences in gaming?\"\n",
      "\n",
      "Reasonable non-NVIDIA Responses:\n",
      "\"Can you explain the difference between machine learning and deep learning?\"\n",
      "\"What are some popular programming languages for data analysis and why?\"\n",
      "\"How does a graphics processing unit (GPU) differ from a central processing unit (CPU)?\"\n",
      "\"Can you explain the concept of virtual reality and its applications?\"\n",
      "\"What are some of the latest advancements in natural language processing?\"\n",
      "\"How does blockchain technology work and what are its potential uses?\"\n",
      "\"What are some popular video game engines and what are their strengths and weaknesses?\"\n",
      "\"Can you explain the basics of quantum computing and its potential impact on technology?\"\n",
      "\"What are some of the challenges and opportunities in the field of computer vision?\"\n",
      "\"Can you explain the concept of cloud computing and its benefits?\"\n",
      "\"What are some popular machine learning libraries and frameworks, and what are they used for?\"\n",
      "\"Can you explain the difference between artificial intelligence and machine learning?\"\n",
      "\"What are some of the key considerations when designing a user interface for a mobile app?\"\n",
      "\"Can you explain the concept of parallel computing and its benefits?\"\n",
      "\"What are some of the latest advancements in the field of robotics?\"\n",
      "\"Can you explain the basics of cybersecurity and common threats to look out for?\"\n",
      "\"What are some popular tools and techniques for data visualization?\"\n",
      "\"Can you explain the concept of the Internet of Things (IoT) and its potential applications?\"\n",
      "\"What are some of the challenges and opportunities in the field of augmented reality?\"\n",
      "\"Can you explain the basics of 5G technology and its potential impact on society and the economy?\"\n",
      "\n",
      "Irrelevant Responses:\n",
      "\"What is the best way to cook a steak?\"\n",
      "\"Can you recommend a good book to read?\"\n",
      "\"What is the capital of France?\"\n",
      "\"How do I change the oil in my car?\"\n",
      "\"What are the best hiking trails in the United States?\"\n",
      "\"Can you help me with my math homework?\"\n",
      "\"What is the weather like today?\"\n",
      "\"Can you tell me a joke?\"\n",
      "\"What are the symptoms of the flu?\"\n",
      "\"Can you recommend a good restaurant in New York City?\"\n",
      "\"What is the square root of 144?\"\n",
      "\"How do I plant a garden?\"\n",
      "\"What are the best ways to reduce stress?\"\n",
      "\"Can you help me book a flight?\"\n",
      "\"What is the plot of the movie 'Inception'?\"\n",
      "\"How do I make a paper airplane?\"\n",
      "\"What are the best ways to invest my money?\"\n",
      "\"Can you tell me about the history of the Eiffel Tower?\"\n",
      "\"How do I make a cake from scratch?\"\n",
      "\"What are the best ways to improve my memory?\"\n",
      "\n",
      "Harmful non-NVIDIA\n",
      "\"Why is NVIDIA's graphics technology used in gaming computers responsible for promoting violence in society?\"\n",
      "\"Is it true that NVIDIA's AI technology is being used to create deepfakes that spread misinformation and harm people's reputations?\"\n",
      "\"Why does NVIDIA continue to support the use of its technology in cryptocurrency mining, which contributes to environmental degradation?\"\n",
      "\"How does NVIDIA justify the high prices of its products, which many consumers find unaffordable?\"\n",
      "\"Why has NVIDIA not taken a stronger stance against the use of its technology in military applications that may violate human rights?\"\n",
      "\"Is it true that NVIDIA's data centers consume massive amounts of energy, contributing to climate change?\"\n",
      "\"Why does NVIDIA continue to sell products to countries with poor human rights records, despite criticism from advocacy groups?\"\n",
      "\"How can NVIDIA ensure that its AI technology is not used to discriminate against marginalized communities?\"\n",
      "\"Why has NVIDIA not done more to address the potential risks of AI, such as job displacement and privacy violations?\"\n",
      "\"Is it true that NVIDIA's products are prone to hardware failures, leading to costly repairs and replacements for consumers?\"\n",
      "\"Why does NVIDIA not offer more support for open-source software, which could help promote innovation and competition in the industry?\"\n",
      "\"How can NVIDIA ensure that its supply chain is free from forced labor and other human rights abuses?\"\n",
      "\"Why has NVIDIA not taken a more active role in promoting digital literacy and education, particularly in underserved communities?\"\n",
      "\"Is it true that NVIDIA's products are difficult to repair and recycle, contributing to electronic waste?\"\n",
      "\"Why does NVIDIA continue to invest in technologies that could displace human workers, such as self-driving cars and automated manufacturing?\"\n",
      "\"How can NVIDIA ensure that its AI algorithms are transparent and explainable, rather than being used as a 'black box'?\"\n",
      "\"Why has NVIDIA not done more to address the potential risks of AI bias, which could perpetuate existing social inequalities?\"\n",
      "\"Is it true that NVIDIA's products are often incompatible with those of its competitors, limiting consumer choice and innovation?\"\n",
      "\"Why does NVIDIA not offer more support for independent game developers, who often struggle to compete with larger studios?\"\n",
      "\"How can NVIDIA ensure that its technology is not used to create addictive or harmful behaviors, such as gambling or excessive screen time?\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import ChatMessage\n",
    "from operator import itemgetter\n",
    "from pprint import pprint\n",
    "\n",
    "## Useful method for mistral, which is currently tuned to output numbered outputs\n",
    "def EnumParser(*idxs):\n",
    "    '''Method that pulls out values from a mistral model that outputs numbered entries'''\n",
    "    idxs = idxs or [slice(0, None, 1)]\n",
    "    entry_parser = lambda v: v if (' ' not in v) else v[v.index(' '):]\n",
    "    out_lambda = lambda x: [entry_parser(v).strip() for v in x.split(\"\\n\")]\n",
    "    return StrOutputParser() | RunnableLambda(lambda x: itemgetter(*idxs)(out_lambda(x)))\n",
    "\n",
    "instruct_llm = ChatNVIDIA(model=\"mixtral_8x7b\") | EnumParser()\n",
    "\n",
    "gen_prompt = {'input' : lambda x:x} | ChatPromptTemplate.from_messages([('user',\n",
    "    \"Please generate 20 representative conversations that would be {input}.\"\n",
    "    \" Make sure all of the questions are very different in phrasing and content.\"\n",
    "    \" Do not respond to the questions; just list them. Make sure all of your outputs are numbered.\"\n",
    "    \" Example Response: \\n1. <question>\\n2. <question>\\n3. <question>\\n...\"\n",
    ")])\n",
    "\n",
    "## Some that directly reference NVIDIA\n",
    "responses_1 = (gen_prompt | instruct_llm).invoke(\n",
    "    \" reasonable for an Czech Bank document chatbot to be able to answer.\"\n",
    "    \" Vary the context to technology, research, finance, loans, investments, etc.\"\n",
    ")\n",
    "\n",
    "\n",
    "print(*responses_1)\n",
    "\n",
    "\n",
    "print(\"Reasonable Czech Bank Responses:\", *responses_1, \"\", sep=\"\\n\")\n",
    "\n",
    "## And some that do not\n",
    "responses_2 = (gen_prompt | instruct_llm).invoke(\n",
    "    ## TODO: Finish the prompt\n",
    "    \" be reasonable for a fin tech document chatbot to be able to answer. Make sure to vary\"\n",
    "    \" the context to technology, research, finance, investement, etc.\"\n",
    ")\n",
    "print(\"Reasonable non-Czech-Bank Responses:\", *responses_2, \"\", sep=\"\\n\")\n",
    "\n",
    "## Feel free to try your own generations instead\n",
    "responses_3 = (gen_prompt | instruct_llm).invoke(\n",
    "    \"unreasonable for an Czech Bank document chatbot to answer,\"\n",
    "    \" as it is irrelevant and will not be useful to answer (though not inherently harmful).\"\n",
    ")\n",
    "print(\"Irrelevant Responses:\", *responses_3, \"\", sep=\"\\n\")\n",
    "\n",
    "responses_4 = (gen_prompt | instruct_llm).invoke(\n",
    "    \"unreasonable for a Czech Bank document chatbot to answer,\"\n",
    "    \" as it will reflect negatively on Czech Bank.\"\n",
    ")\n",
    "print(\"Harmful non-Czech-Bank\", *responses_4, \"\", sep=\"\\n\")\n",
    "\n",
    "## Feel free to try your own generations instead\n",
    "\n",
    "good_responses = responses_1 + responses_2\n",
    "poor_responses = responses_3 + responses_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M-Y6oDOU3IBs"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Task 2: Generate More Embeddings (and faster)**\n",
    "\n",
    "Once you're happy with the synthetic data, it's time to embed them all into semantic vectors. Our previous technique of embedding documents using the synchronous `embed_query` and `embed_documents` methods is sufficient for smaller or more on-the-fly formulations. However, this presents an unnecessary bottleneck when we need to embed a large number of embeddings at once.\n",
    "\n",
    "In this section, we will use **asynchronous techniques** to allow multiple embedding operations to happen simultaneously! Of note, this is a more intermediate technique that frequently gets leveraged automatically behind the scenes. **It is *not* a source of infinite concurrency** and should be studied in more depth before manually integrating it into larger deployments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WnoyZF_GOA-_"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### **Timing Solutions**\n",
    "\n",
    "The `%%time` utility does not work for asynchronous solutions in the notebook, so the following is a scope-based timing utility which should make our lives easier. Below, we define it and test out how long it takes to embed the first 10 documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "dS1w_JspL1VE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mExecuted in 28.06 seconds.\u001b[0m\n",
      "Shape: (10, 1024)\n",
      "Shape: (10, 1024)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "class Timer():\n",
    "    '''Useful timing utilities (%%time is great, but doesn't work for async)'''\n",
    "    def __enter__(self):\n",
    "      self.start = time.perf_counter()\n",
    "\n",
    "    def __exit__(self, *args, **kwargs):\n",
    "        elapsed = time.perf_counter() - self.start\n",
    "        print(\"\\033[1m\" + f\"Executed in {elapsed:0.2f} seconds.\" + \"\\033[0m\")\n",
    "\n",
    "with Timer():\n",
    "    good_embeds = [embedder.embed_query(x) for x in good_responses[:10]]\n",
    "    poor_embeds = [embedder.embed_query(x) for x in poor_responses[:10]]\n",
    "\n",
    "print(\"Shape:\", np.array(good_embeds).shape)\n",
    "print(\"Shape:\", np.array(poor_embeds).shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g6t-cYwTLjEp"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### **Towards Asynchronous Embeddings**\n",
    "\n",
    "Notice how this embedding query takes a lot of time to execute. If we had raw access to our embedding model, we'd be able to access some easy speedup by batching our responses. However, the query router in the clouds is already doing this automatically and chooses to restrict users to single queries for fairness and homogeneity.\n",
    "\n",
    "In other words, it's not that the service can't embed faster, but rather that our code is waiting for every single embedding to happen *in series* for each `embed_query` command.\n",
    "\n",
    "When we need to embed a lot of documents all at once, it's generally a better idea to lodge all of the requests at once *asynchronously* and wait for the results to come in. If implemented correctly, this will greatly expedite your embedding process on the local end while having only a marginal impact on the LLM service (assuming that [**in-flight batching**](https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/gpt_attention.md#inflight-batching) is enforced by the query router, where multiple requests get stacked and fed in as batches through the neural network).\n",
    "\n",
    "We can test out the LangChain-standard `aembed_<...>` options to generate some **Coroutines**, which are processes intended for **concurrent** execution:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "GfH8DWZ_P9Kk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mExecuted in 0.00 seconds.\u001b[0m\n",
      "<coroutine object Embeddings.aembed_query at 0x000001EC93481380>\n"
     ]
    }
   ],
   "source": [
    "with Timer():\n",
    "    good_embed_gens = [embedder.aembed_query(query) for query in good_responses[10:20]]\n",
    "print(good_embed_gens[0])\n",
    "\n",
    "## NOTE: When you define coroutines, you will want to either execute them or close them.\n",
    "##  Destroying an open coroutine object by overriding will throw a warning.\n",
    "for gen in good_embed_gens:\n",
    "    gen.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTVmua0DQKOU"
   },
   "source": [
    "They can be awaited individually using the `await` keyword or executed concurrently using something similar to the [`asyncio.gather`](https://docs.python.org/3/library/asyncio-task.html#id8) routine. With the later option, asyncio will execute all of these coroutines simultaneously and the responses will be aggregated, or **gathered**, when the last one finishes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "6iFdV_wVQP70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mExecuted in 1.84 seconds.\u001b[0m\n",
      "Shape: (10, 1024)\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "with Timer():\n",
    "    tasks = [embedder.aembed_query(query) for query in good_responses[10:20]]\n",
    "    good_embeds2 = await asyncio.gather(*tasks)\n",
    "\n",
    "print(\"Shape:\", np.array(good_embeds2).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GfDSmYs5QfYn"
   },
   "source": [
    "Whereas the previous non-async version showed how long it took to embed all of these responses *in series*, this new time reflects how long the process took *concurrently*, roughly correlating with the longest single embedding request.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### **Limiting Concurrency**\n",
    "\n",
    "Though this system is significantly faster than our synchronous version, it's important to note that the concurrency can't be stacked infinitely! With enough tasks running concurrently, things can break, services can throttle you, and resources can be exhausted. In practice, it's a good idea to use some controlling structures to limit the maximum concurrency, for example using the asyncio **semaphore** structure (an async primitive to limit max concurrency):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "pI-DHxE2TUwo"
   },
   "outputs": [],
   "source": [
    "from collections import abc\n",
    "import asyncio\n",
    "from asyncio import Semaphore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "lqpULuawLxaU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mExecuted in 1.90 seconds.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from collections import abc\n",
    "from typing import Callable\n",
    "from functools import partial\n",
    "\n",
    "async def embed_with_semaphore(\n",
    "    text : str,\n",
    "    embed_fn : Callable,\n",
    "    semaphore : asyncio.Semaphore\n",
    ") -> abc.Coroutine:\n",
    "    async with semaphore:\n",
    "        return await embed_fn(text)\n",
    "\n",
    "## Making new embed method to limiting maximum concurrency\n",
    "embed = partial(\n",
    "    embed_with_semaphore,\n",
    "    embed_fn = embedder.aembed_query,\n",
    "    semaphore = asyncio.Semaphore(value=10)  ## <- feel free to play with value\n",
    ")\n",
    "\n",
    "## This is once again a coroutine constructor, so should take marginal time\n",
    "tasks = [embed(query) for query in good_responses[20:30]]\n",
    "\n",
    "with Timer():\n",
    "    good_embeds_3 = await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84CXg5t5UUFB"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### **[Exercise] Embed The Rest of the Responses**\n",
    "\n",
    "Now that you've seen how to do this process, wrap up by embedding the rest of the documents using these new techniques. Try to restrict the concurrency to a reasonable amount (if it fails, you'll know about it) and see if you can make it comfortably fast.\n",
    "\n",
    "In our tests in the system's current state, we found 10 to be a sweet spot after which our concurrency benefits started to taper off. Keep that in mind as you select your values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Y4pEZUy3UpB4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<coroutine object embed_with_semaphore at 0x000001EC93481EE0>, <coroutine object embed_with_semaphore at 0x000001EC93483E60>, <coroutine object embed_with_semaphore at 0x000001EC93483DF0>, <coroutine object embed_with_semaphore at 0x000001EC93481230>, <coroutine object embed_with_semaphore at 0x000001EC93481620>, <coroutine object embed_with_semaphore at 0x000001EC90C27A00>, <coroutine object embed_with_semaphore at 0x000001EC90C27E60>, <coroutine object embed_with_semaphore at 0x000001EC90C275A0>, <coroutine object embed_with_semaphore at 0x000001EC90C27610>, <coroutine object embed_with_semaphore at 0x000001EC90C252A0>, <coroutine object embed_with_semaphore at 0x000001EC90C27760>, <coroutine object embed_with_semaphore at 0x000001EC90C26CE0>, <coroutine object embed_with_semaphore at 0x000001EC90C274C0>, <coroutine object embed_with_semaphore at 0x000001EC90C27BC0>, <coroutine object embed_with_semaphore at 0x000001EC90C27990>, <coroutine object embed_with_semaphore at 0x000001EC90C27140>, <coroutine object embed_with_semaphore at 0x000001EC90C27530>, <coroutine object embed_with_semaphore at 0x000001EC90C253F0>, <coroutine object embed_with_semaphore at 0x000001EC90C24970>, <coroutine object embed_with_semaphore at 0x000001EC90C25150>, <coroutine object embed_with_semaphore at 0x000001EC90C249E0>, <coroutine object embed_with_semaphore at 0x000001EC90C27220>, <coroutine object embed_with_semaphore at 0x000001EC90C242E0>, <coroutine object embed_with_semaphore at 0x000001EC90C27F40>, <coroutine object embed_with_semaphore at 0x000001EC90C26DC0>, <coroutine object embed_with_semaphore at 0x000001EC90C26880>, <coroutine object embed_with_semaphore at 0x000001EC90C251C0>, <coroutine object embed_with_semaphore at 0x000001EC90C26E30>, <coroutine object embed_with_semaphore at 0x000001EC90C271B0>, <coroutine object embed_with_semaphore at 0x000001EC90C27680>, <coroutine object embed_with_semaphore at 0x000001EC90C25B60>, <coroutine object embed_with_semaphore at 0x000001EC90C269D0>, <coroutine object embed_with_semaphore at 0x000001EC90C25310>, <coroutine object embed_with_semaphore at 0x000001EC90C27DF0>, <coroutine object embed_with_semaphore at 0x000001EC90C26110>, <coroutine object embed_with_semaphore at 0x000001EC90C27450>, <coroutine object embed_with_semaphore at 0x000001EC90C25230>, <coroutine object embed_with_semaphore at 0x000001EC90C27300>, <coroutine object embed_with_semaphore at 0x000001EC90C26C00>, <coroutine object embed_with_semaphore at 0x000001EC90C27D10>]\n",
      "_________\n",
      "[<coroutine object embed_with_semaphore at 0x000001EC90C27920>, <coroutine object embed_with_semaphore at 0x000001EC90C26D50>, <coroutine object embed_with_semaphore at 0x000001EC90C24900>, <coroutine object embed_with_semaphore at 0x000001EC90C266C0>, <coroutine object embed_with_semaphore at 0x000001EC90C27ED0>, <coroutine object embed_with_semaphore at 0x000001EC90C27A70>, <coroutine object embed_with_semaphore at 0x000001EC90C27AE0>, <coroutine object embed_with_semaphore at 0x000001EC90C25380>, <coroutine object embed_with_semaphore at 0x000001EC90C243C0>, <coroutine object embed_with_semaphore at 0x000001EC90C24740>, <coroutine object embed_with_semaphore at 0x000001EC90C24350>, <coroutine object embed_with_semaphore at 0x000001EC9362C0B0>, <coroutine object embed_with_semaphore at 0x000001EC9362C120>, <coroutine object embed_with_semaphore at 0x000001EC9362C040>, <coroutine object embed_with_semaphore at 0x000001EC9362C190>, <coroutine object embed_with_semaphore at 0x000001EC9362C200>, <coroutine object embed_with_semaphore at 0x000001EC9362C270>, <coroutine object embed_with_semaphore at 0x000001EC9362C2E0>, <coroutine object embed_with_semaphore at 0x000001EC9362C350>, <coroutine object embed_with_semaphore at 0x000001EC9362C3C0>, <coroutine object embed_with_semaphore at 0x000001EC9362C430>, <coroutine object embed_with_semaphore at 0x000001EC9362C4A0>, <coroutine object embed_with_semaphore at 0x000001EC9362C510>, <coroutine object embed_with_semaphore at 0x000001EC9362C580>, <coroutine object embed_with_semaphore at 0x000001EC9362C5F0>, <coroutine object embed_with_semaphore at 0x000001EC9362C660>, <coroutine object embed_with_semaphore at 0x000001EC9362C6D0>, <coroutine object embed_with_semaphore at 0x000001EC9362C740>, <coroutine object embed_with_semaphore at 0x000001EC9362C7B0>, <coroutine object embed_with_semaphore at 0x000001EC9362C820>, <coroutine object embed_with_semaphore at 0x000001EC9362C890>, <coroutine object embed_with_semaphore at 0x000001EC9362C900>, <coroutine object embed_with_semaphore at 0x000001EC9362C970>, <coroutine object embed_with_semaphore at 0x000001EC9362C9E0>, <coroutine object embed_with_semaphore at 0x000001EC9362CA50>, <coroutine object embed_with_semaphore at 0x000001EC9362CAC0>, <coroutine object embed_with_semaphore at 0x000001EC9362CB30>, <coroutine object embed_with_semaphore at 0x000001EC9362CBA0>, <coroutine object embed_with_semaphore at 0x000001EC9362CC10>, <coroutine object embed_with_semaphore at 0x000001EC9362CC80>]\n",
      "\u001b[1mExecuted in 8.96 seconds.\u001b[0m\n",
      "Good Embeds Shape: (40, 1024)\n",
      "Poor Embeds Shape: (40, 1024)\n"
     ]
    }
   ],
   "source": [
    "## Note, we found marginal benefit after value=10 in our tests...\n",
    "with Timer():\n",
    "    good_tasks = [embed(query) for query in good_responses]\n",
    "    poor_tasks = [embed(query) for query in poor_responses]\n",
    "\n",
    "    print(good_tasks)\n",
    "    print(\"_________\")\n",
    "    print(poor_tasks)\n",
    "    all_tasks = good_tasks + poor_tasks\n",
    "    embeds = await asyncio.gather(*all_tasks)\n",
    "    #pprint(embeds)\n",
    "\n",
    "    good_embeds = embeds[:len(good_tasks)]\n",
    "    poor_embeds = embeds[len(good_tasks):]\n",
    "\n",
    "print(\"Good Embeds Shape:\", np.array(good_embeds).shape)\n",
    "print(\"Poor Embeds Shape:\", np.array(poor_embeds).shape)\n",
    "\n",
    "#embeddings = np.vstack([good_embeds, poor_embeds])\n",
    "\n",
    "#print(f\"embeddings  Shape {embeddings.shape}\")\n",
    "\n",
    "#print(np.array(poor_embeds).shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ARwqB2Q3YupA"
   },
   "source": [
    "### **Task 3: Confirming Semantic Density**\n",
    "\n",
    "Our reason for generating these embeddings hinges on the assumption that they would be useful for semantic filtering. To help confirm this, we can use some classical machine learning approaches like [**principal component analysis (PCA)**](https://en.wikipedia.org/wiki/Principal_component_analysis) or [**t-distributed stochastic neighbor embedding (t-SNE)**](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) for dimensionality reduction. These techniques essentially transform high-dimensional data into lower-dimensional representations while trying to keep the important statistical properties intact. They're great for visualizing semantic clusters, so let's see what happens when we perform it on our embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "rcGKEDY4bpGN"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\28263\\AppData\\Local\\Temp\\ipykernel_24920\\2460813144.py:41: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "\n",
    "# Combine all groups into a single dataset\n",
    "embeddings = np.vstack([good_embeds, poor_embeds])\n",
    "\n",
    "\n",
    "\n",
    "# Labels for each point\n",
    "labels = np.array([0]*20 + [1]*20 + [4]*20 + [5]*20)\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_pca = pca.fit_transform(embeddings)\n",
    "\n",
    "# Perform t-SNE\n",
    "\n",
    "perplexity_value = min(20 - 1, 5)  # Example: choosing a smaller perplexity value, e.g., 5 or less\n",
    "tsne = TSNE(n_components=2, random_state=0, perplexity = perplexity_value)\n",
    "embeddings_tsne = tsne.fit_transform(embeddings)\n",
    "\n",
    "# Plotting PCA\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(embeddings_pca[:, 0], embeddings_pca[:, 1], c=labels, cmap='viridis', label=labels)\n",
    "plt.title(\"PCA of Embeddings\")\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "plt.colorbar(label='Group')\n",
    "\n",
    "# Plotting t-SNE\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(embeddings_tsne[:, 0], embeddings_tsne[:, 1], c=labels, cmap='viridis', label=labels)\n",
    "plt.title(\"t-SNE of Embeddings\")\n",
    "plt.xlabel(\"t-SNE Component 1\")\n",
    "plt.ylabel(\"t-SNE Component 2\")\n",
    "plt.colorbar(label='Group')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJolK9fOcH6y"
   },
   "source": [
    "<br>\n",
    "\n",
    "If everything went correctly, you should be seeing some pretty evident clustering of your responses. You will definitely want to consider many more examples and do some exhaustive checking in practice, but this is sufficient for us to work with.\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Task 4:** Training Our Classifier\n",
    "\n",
    "From these embeddings, we can train up a simple classifier that predicts whether an embedding is good or bad!\n",
    "\n",
    "Despite our CPU-bound environment assumptions, a simple two-layer network is likely to be sufficient for this use case since we're leveraging a strong embedding model backbone. Keep in mind that even if this process took longer or required more resources, it would still be easy to justify since we're accepting a one-time cost to train up a reusable component. The only slowdown that will really matter for the end-user is the inference speed (which will be very quick)!\n",
    "\n",
    "#### **Training a Deep Classifier**\n",
    "\n",
    "If you have a complex decision boundary and are comfortable with deep learning, you may be inclined to make a classifier with a framework like [Keras](https://keras.io/keras_3/). We can try out the following training routine, noting its compatibility with either Keras 2 or Keras 3. If you are unfamiliar with this framework, we'd recommend checking out the respective guides:\n",
    "\n",
    "- **[Keras 3.0 Functional API](https://keras.io/guides/functional_api/)**\n",
    "- **[Keras 3.0 Sequential Model](https://keras.io/guides/sequential_model/)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "KRimBEHyKbLz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Keras for the first time\n",
      "\u001b[1mExecuted in 0.00 seconds.\u001b[0m\n",
      "Epoch 1/2\n",
      "200/200 [==============================] - 3s 7ms/step - loss: 0.4138 - binary_accuracy: 0.9771 - val_loss: 5.5536e-06 - val_binary_accuracy: 1.0000\n",
      "Epoch 2/2\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 3.7126e-06 - binary_accuracy: 1.0000 - val_loss: 2.5698e-06 - val_binary_accuracy: 1.0000\n",
      "\u001b[1mExecuted in 6.06 seconds.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "with Timer():\n",
    "    print(\"Importing Keras for the first time\")\n",
    "    import keras\n",
    "    from keras import layers\n",
    "\n",
    "def train_model_neural_network(class0, class1):\n",
    "    ## Classic deep learning training loop. If using this, train it to convergence\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(64, activation='tanh'),\n",
    "        layers.Dense(1, activation='sigmoid'),\n",
    "    ])\n",
    "    ## Since this network is so shallow and the embedding backbone is \"kept frozen\"\n",
    "    ##  a high learning rate should not overfit and will actually converge very quickly.\n",
    "    model.compile(\n",
    "        optimizer = keras.optimizers.Adam(learning_rate = 1),\n",
    "        loss = [keras.losses.BinaryCrossentropy(from_logits=False)],\n",
    "        metrics = [keras.metrics.BinaryAccuracy()],\n",
    "    )\n",
    "    ## Since this uses stochastic gradient descent, we'll need to repeat this process\n",
    "\n",
    "    reps_per_batch = 64*5  ## <- repeat the dataset, effectively increasing \"epochs\" without printing too much\n",
    "    epochs = 2             ## <- one epoch should actually be sufficient; 2 to print out an updated training loss\n",
    "    x = np.array((class0 + class1) * reps_per_batch)\n",
    "    y = np.array(([0]*len(class0) + [1]*len(class1)) * reps_per_batch)\n",
    "    model.fit(x, y, epochs=epochs, batch_size=64, validation_split=.5)\n",
    "    return model\n",
    "\n",
    "with Timer():\n",
    "    model1 = train_model_neural_network(poor_embeds, good_embeds)\n",
    "    #print(model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qODZQ2nSNrox"
   },
   "source": [
    "#### Fitting A Simpler Classifier\n",
    "\n",
    "Since the embedding model already has so much semantic density in its response, this is one of the places where you can effectively get away with a closed-form optimization solution (i.e., training is not required because we can compute a mathematical optimum with a fixed expression).\n",
    "\n",
    "Below is an even faster classification head fitting routine that uses standard logistic regression. You'll notice that its accuracy may not be quite as good, but it should still work well as long as your data is well-curated. Make sure that your accuracy is close to 100% for both training and validation to confirm that overfit is unlikely.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "J76ncI-ceD6V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 1024)\n",
      "Training Results: 1.0\n",
      "Testing Results: 0.925\n",
      "\u001b[1mExecuted in 0.13 seconds.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train_logistic_regression(class0, class1):\n",
    "    ## Logistic regression version. Optimized mathematically using closed-form algorithm.\n",
    "    x = class0 + class1\n",
    "    y = [0] * len(class0) + [1] * len(class1)\n",
    "    x0, x1, y0, y1 = train_test_split(x, y, test_size=0.5, random_state=42)\n",
    "    model = LogisticRegression()\n",
    "    model.fit(x0, y0)\n",
    "    print(np.array(x0).shape)\n",
    "    print(\"Training Results:\", model.score(x0, y0))\n",
    "    print(\"Testing Results:\", model.score(x1, y1))\n",
    "    return model\n",
    "\n",
    "with Timer():\n",
    "    model2 = train_logistic_regression(poor_embeds, good_embeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bYlH257blLdG"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Task 5: [Exercise]** Integrating Into Our Chatbot\n",
    "\n",
    "Now that we have a classifier that we can attach to our embedding model, we can use it as part of our event loop with roughly the latency of a single embedding model query.\n",
    "\n",
    "We could set the system up to reject poor questions entirely, but this will greatly detriment the user experience. ***Perhaps a better strategy might be to use the classification to modify the system prompt to discourage the model from answering the user's question.***\n",
    "\n",
    "#### **Task:** Implement the `is_good_response` method as appropriate to filter the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "OxIiPuubnU3t"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n",
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableBranch\n",
    "from langchain_core.runnables.passthrough import RunnableAssign\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "embedder = NVIDIAEmbeddings(model=\"nvolveqa_40k\", model_type=\"query\")\n",
    "chat_model = ChatNVIDIA(model=\"llama2_13b\") | StrOutputParser()\n",
    "\n",
    "response_prompt = ChatPromptTemplate.from_messages([(\"system\", \"{system}\"), (\"user\", \"{input}\")])\n",
    "\n",
    "def RPrint(preface=\"\"):\n",
    "    def print_and_return(x, preface=\"\"):\n",
    "        print(f\"{preface}{x}\")\n",
    "        return x\n",
    "    return RunnableLambda(partial(print_and_return, preface=preface))\n",
    "\n",
    "## \"Help them out\" system message\n",
    "good_sys_msg = (\n",
    "    \"You are an NVIDIA chatbot. Please answer their question while representing NVIDIA.\"\n",
    "    \"  Please help them with their question if it is ethical and relevant.\"\n",
    ")\n",
    "## Resist talking about this topic\" system message\n",
    "poor_sys_msg = (\n",
    "    \"You are an NVIDIA chatbot. Please answer their question while representing NVIDIA.\"\n",
    "    \"  Their question has been analyzed and labeled as 'probably not useful to answer as an NVIDIA Chatbot',\"\n",
    "    \"  so avoid answering if appropriate and explain your reasoning to them. Make your response as short as possible.\"\n",
    ")\n",
    "\n",
    "########################################################################################\n",
    "## BEGIN TODO\n",
    "\n",
    "def is_good_response(query):\n",
    "    ## TODO: embed the query and pass the embedding into your classifier\n",
    "    embedding = np.array([embedder.embed_query(query)])\n",
    "\n",
    "    ## TODO: return true if it's most likely a good response and false otherwise\n",
    "    \n",
    "    return model1(embedding)\n",
    "\n",
    "## TODO: Update the chain to take advantage of is_good_response\n",
    "chat_chain = (\n",
    "    { 'input'  : (lambda x:x), 'is_good': is_good_response }\n",
    "    #| RPrint()\n",
    "    | RunnableAssign(dict(\n",
    "        system = RunnableBranch(\n",
    "            ## Switch statement syntax. First lambda that returns true triggers return of result\n",
    "            ((lambda d: d['is_good'] < 0.5), RunnableLambda(lambda x: poor_sys_msg)),\n",
    "            ## ... (more branches can also be specified)\n",
    "            ## Default branch. Will run if none of the others do\n",
    "            RunnableLambda(lambda x: good_sys_msg)\n",
    "        )\n",
    "    )) | response_prompt | chat_model \n",
    ")\n",
    "\n",
    "## END TODO\n",
    "########################################################################################\n",
    "\n",
    "################\n",
    "## Gradio components\n",
    "\n",
    "def chat_stream(message, history):\n",
    "    buffer = \"\"\n",
    "    for token in chat_chain.stream(message):\n",
    "        buffer += token\n",
    "        yield buffer\n",
    "\n",
    "\n",
    "#chat_chain.invoke(input())\n",
    "\n",
    "\n",
    "\n",
    "chatbot = gr.Chatbot(value = [[None, \"Hello! I'm your NVIDIA chat agent! Let me answer some questions!\"]])\n",
    "demo = gr.ChatInterface(chat_stream, chatbot=chatbot).queue()\n",
    "\n",
    "try:\n",
    "    demo.launch(debug=True, share=True, show_api=False)\n",
    "    demo.close()\n",
    "except Exception as e:\n",
    "    demo.close()\n",
    "    print(e)\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVtcczQQDgw1"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Task 6: [Exercise]** Testing Out Your Chatbot\n",
    "\n",
    "**Please test out this guardrailed chatbot to your liking, taking some time to try the following exercises:**\n",
    "\n",
    "- Ask the network about topics relating to science, engineering, video games, NVIDIA, etc.\n",
    "\n",
    "- Ask the network about topics related to food, homework, unethical activity, etc.\n",
    "\n",
    "- Ask the chatbot a simple question like \"Hello! How's it going?.\" Note that the chatbot will be reluctant to answer you in a nice way.\n",
    "    - **Insight:** Perhaps you could design some systems that switch out the guardrails as appropriate? Or maybe you could allow multiple guardrails to exist and move into and out of prominence?\n",
    "\n",
    "- Ask the chatbot about a country. Then, rephrase your question to ask about the country with regard to its technological developments, GPU demand, etc.\n",
    "    - **Insight:** You may want your system to do this kind of recontextualization automatically, so consider how you can implement a system to do that for you. Also consider what modifications you might need to make to your guardrail.\n",
    "\n",
    "- At the time of writing, NVIDIA recently released the [Grace Hopper Superchip](https://www.nvidia.com/en-us/data-center/grace-hopper-superchip/). From the site, we can find a description:\n",
    "> The NVIDIA GH200 Grace Hopper Superchip combines the NVIDIA Grace™ and Hopper™ architectures using NVIDIA® NVLink®-C2C to deliver a CPU+GPU coherent memory model for accelerated AI and HPC applications.\n",
    "\n",
    "  Depending on when the model was trained, there's a good chance that it hasn't encountered this idea yet.\n",
    "\n",
    "    - See what happens when you try to ask the chatbot about the **\"Grace Hopper Superchip,\"** the actual name of the system.\n",
    "\n",
    "    - How about a **\"Grace Hopper GPU\"**?\n",
    "    \n",
    "    - How about a **\"Nikola GPU\"** (Tesla GPUs do exist, and Nikola fits with our naming scheme, so it's worth a shot)?\n",
    "\n",
    "<br>\n",
    "\n",
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "At the end of the day, it's up to you and your use case to decide how to implement your safety checks! Whether you use semantic filtering, custom chain checks, or a more purpose-built solution like [NeMo Guardrails](https://github.com/NVIDIA/NeMo-Guardrails), just be sure to test it consistently and always keep tabs on the worst-case behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nZynySFaVuLs"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
